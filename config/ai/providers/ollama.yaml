# Ollama Provider Configuration
# 로컬 LLM 런타임 (Go 기반)
#
# 설치: https://ollama.ai
# 모델 다운로드: ollama pull llama3.1:8b-instruct-q8_0

type: "local_runtime"
runtime: "ollama"

# 모델 설정
model: "llama3.1:8b-instruct-q8_0"

# Ollama 옵션
options:
  num_predict: 2048
  num_ctx: 8192
  temperature: 0.3
  top_p: 0.9
  repeat_penalty: 1.1

# 서버 주소는 network_config.yaml에서 참조
# server: "localhost"  # → network_config.ai_servers.localhost

# === 테스트된 모델들 ===
# llama3.1:8b-instruct-q8_0  - 권장 (6.4초, 한국어 우수)
# llama3.2                    - 기본 (언어 혼합 문제)
# gemma2:27b-instruct-q4_K_M  - 7.3초 (영어 우세)
# llama4-scout-*              - 느림 (1분 40초, 24GB VRAM 부족)
