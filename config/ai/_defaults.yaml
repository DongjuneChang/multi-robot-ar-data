# AI Configuration Defaults
# 공통 구조 정의 - providers/*.yaml에서 덮어씀
#
# 작성일: 2025-12-01
# 작성자: Dongjune Chang
#
# === Provider Types ===
# local_runtime : 로컬에서 모델 직접 실행
#                 - ollama: Go 기반 런타임 (권장, 가장 쉬움)
#                 - llama.cpp: C++ 추론 엔진 (Ollama가 내부적으로 사용)
#                 - vllm: Python 서빙 프레임워크 (고성능 배치 처리)
#
# cloud_api     : 클라우드 API 호출
#                 - openai: OpenAI API (GPT-4, GPT-4o, o1, ...)
#                 - anthropic: Anthropic API (Claude 3, 3.5, 4, ...)
#                 - google: Google AI API (Gemini 1.5, 2.0, ...)
#
# === API Keys ===
# 클라우드 API 키는 .env 파일에 저장 (gitignore 됨)
# 예: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY
#

# === Riley 기본 설정 ===
riley:
  wake_word: "Riley"
  listening_duration: 5.0
  max_recording_duration: 10.0

# === LLM 공통 설정 ===
llm:
  timeout: 60
  max_tokens: 2048
  temperature: 0.3
  top_p: 0.9

# === Provider 기본 구조 (Override Required) ===
provider:
  type: ""              # OVERRIDE: "local_runtime" or "cloud_api"
  runtime: ""           # OVERRIDE (local_runtime): "ollama", "llama.cpp", "vllm"
  model: ""             # OVERRIDE: 모델 이름
  # api_key_env: ""     # cloud_api: 환경변수 이름 (예: "ANTHROPIC_API_KEY")
  # server: ""          # network_config.yaml의 ai_servers 키 참조
