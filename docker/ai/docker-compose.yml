# Multi-HRI AI Server Docker Compose
# RILEY AI + ChromaDB (Conversation History)
#
# 사용법:
#   1. .env 파일에 로컬 경로 설정 (선택사항)
#   2. docker-compose up --build -d
#
# GPU PC에서 실행 후 ngrok으로 외부 노출:
#   ngrok http 5001 --domain=riley.multi-hri.ngrok.app

version: '3.8'

services:
  ai-server:
    build: .
    container_name: multi-hri-ai
    ports:
      - "5001:5000"  # AI Server (Web Server와 포트 충돌 방지)
    volumes:
      # Shared data config (read-only)
      - ${SHARED_DATA_PATH:-../..}/config:/shared_data/config:ro
      # AI config (personas, providers, experts)
      - ./config:/app/config:ro
      # ChromaDB cache
      - chroma-cache:/root/.cache/chroma
    environment:
      - PYTHONUNBUFFERED=1
      - SHARED_DATA_DIR=/shared_data/config
      - OLLAMA_HOST=${OLLAMA_HOST:-host.docker.internal:11434}
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Linux compatibility
    networks:
      - multi-hri-ai-net
    restart: unless-stopped

  # ChromaDB (Vector Database for conversation history)
  chromadb:
    image: chromadb/chroma:latest
    container_name: multi-hri-chromadb
    ports:
      - "8000:8000"
    volumes:
      - ./chroma_data:/data
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    networks:
      - multi-hri-ai-net
    restart: unless-stopped

  # Ollama (uncomment if not running on host)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: multi-hri-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   networks:
  #     - multi-hri-ai-net

networks:
  multi-hri-ai-net:
    driver: bridge

volumes:
  chroma-cache:
  # ollama-data:
