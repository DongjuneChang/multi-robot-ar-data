# AI Configuration for Multi-HRI System
# RILEY AI 및 LLM 관련 설정
# 작성일: 2025-11-27

# === RILEY AI Server ===
# GPU PC + ngrok으로 외부 노출
# Docker: docker/ai/docker-compose.yml
ai_server:
  port: 5001
  ngrok_domain: "riley.multi-hri.ngrok.app"

# === RILEY 기본 설정 ===
riley:
  wake_word: "Riley"
  listening_duration: 5.0

# === 연결 모드 ===
# 각 사용자 yaml에서 riley.mode로 선택
# NOTE: 실제 IP는 secrets/network_secrets.yaml 참조
modes:
  zmq_local:
    type: "zmq"
    host: "localhost"
    publish_port: 5565    # Unity → Python (음성 명령)
    subscribe_port: 5566  # Python → Unity (LLM 응답)
    ollama_host: "localhost"
    ollama_port: 11434
    model: "llama3.2:1b"

  zmq_remote:
    type: "zmq"
    host: "${RILEY_REMOTE_HOST}"  # secrets/network_secrets.yaml
    publish_port: 5565
    subscribe_port: 5566
    ollama_host: "${RILEY_REMOTE_HOST}"  # secrets/network_secrets.yaml
    ollama_port: 11434
    model: "llama3.2:3b"

  http_ngrok:
    type: "http"
    endpoint: "https://riley.multi-hri.ngrok.app"
    api_key: ""  # 필요시 설정
    model: "llama3.2:70b"

# === Personas ===
# 각 사용자 yaml에서 riley.personas 리스트로 선택
# persona 파일: docker/ai/config/personas/{name}.yaml
personas:
  basic:
    description: "기본 대화 및 로봇 제어"
    file: "basic.yaml"

  robot_expert:
    description: "로봇 동작 및 기구학 전문"
    file: "robot_expert.yaml"

  safety_expert:
    description: "안전 가이드라인 및 위험 감지"
    file: "safety_expert.yaml"

  navigation_expert:
    description: "경로 계획 및 공간 인식"
    file: "navigation_expert.yaml"

# === LLM Providers ===
providers:
  ollama:
    type: "local"
    base_url: "http://localhost:11434"

  openai:
    type: "api"
    base_url: "https://api.openai.com/v1"
    # API key는 환경변수 OPENAI_API_KEY 사용

  anthropic:
    type: "api"
    base_url: "https://api.anthropic.com/v1"
    # API key는 환경변수 ANTHROPIC_API_KEY 사용
